{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 42\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = {'half_life':'logHL', 'clearance':'logCL'}\n",
    "features = ['ecfp4', 'rdkit_ecfp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint in endpoints:\n",
    "    for feature in features:\n",
    "        print(f'\\n\\n ### {endpoint} --> {feature} ### \\n')\n",
    "        \n",
    "        # Load data\n",
    "        endpoint_df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', endpoint, f'{endpoints[endpoint]}_{feature}_dataset.tsv'), sep='\\t')\n",
    "\n",
    "        # Get molecule counts per source\n",
    "        source_counts = {}\n",
    "        sources_list = [item for item in endpoint_df['ref'].unique().tolist() if ',' not in item]\n",
    "        for source in sources_list:\n",
    "            source_mols = endpoint_df.loc[endpoint_df['ref'].str.contains(source, regex=False)]\n",
    "            source_counts[source] = len(source_mols)\n",
    "        print(f'Molecule counts per source: {source_counts}')\n",
    "\n",
    "        # Sort sources by number of molecules (smallest to largest)\n",
    "        sources_list_sorted = [source for source, _ in sorted(source_counts.items(), key=lambda item: item[1])]\n",
    "        print(f'Sorted sources: {sources_list_sorted}')\n",
    "\n",
    "        # Get the set of molecules for each fold\n",
    "        folds_mol_set = {}\n",
    "        for i, source in enumerate(sources_list_sorted):\n",
    "            # Get the data for the given source\n",
    "            source_data = endpoint_df.loc[endpoint_df['ref'].str.contains(source, regex=False)]\n",
    "            if i == 0: # smaller source\n",
    "                # Suffle the data\n",
    "                source_data_shuffled = source_data.sample(frac=1, random_state=seed)\n",
    "                # Split into 5 folds\n",
    "                folds = np.array_split(source_data_shuffled, 5)\n",
    "                # Save each fold inchikey set\n",
    "                for i, fold in enumerate(folds):\n",
    "                    folds_mol_set[f'fold{i+1}'] = set(fold['inchikey'].tolist())\n",
    "            else: # other sources\n",
    "                # Exclude already included molecules from the source data\n",
    "                excluded_mols = [mol for test_mols in folds_mol_set.values() for mol in test_mols]\n",
    "                source_data_filtered = source_data.loc[~source_data['inchikey'].isin(excluded_mols)]\n",
    "                for fold, inchikey_set in folds_mol_set.items():\n",
    "                    # Extract the source molecules already included in the given fold\n",
    "                    previous_mols = inchikey_set.intersection(set(source_data['inchikey'].tolist()))\n",
    "                    # Add source molecules up to the corresponding proportion (20%)\n",
    "                    n_mols_to_add = round(len(source_data)*0.2) - len(previous_mols)\n",
    "                    if n_mols_to_add > 0:\n",
    "                        if fold == 'fold5':\n",
    "                            n_mols_to_add = len(source_data_filtered)\n",
    "                        mols_to_add = source_data_filtered.sample(n=n_mols_to_add, random_state=seed)\n",
    "                        # Remove added molecules from the remaining source data\n",
    "                        source_data_filtered = source_data_filtered.loc[~source_data_filtered['inchikey'].isin(mols_to_add['inchikey'].tolist())]\n",
    "                        # Update fold inchikey set\n",
    "                        folds_mol_set[fold] = inchikey_set.union(set(mols_to_add['inchikey'].tolist()))\n",
    "\n",
    "        # Save each data source fold data in a TSV file\n",
    "        cv_sets_directory = os.path.join(os.getcwd(), '..', 'data', endpoint, 'cv_sets')\n",
    "        if not os.path.exists(cv_sets_directory):\n",
    "            os.makedirs(cv_sets_directory)\n",
    "        if not os.path.exists(os.path.join(cv_sets_directory, feature)):\n",
    "            os.makedirs(os.path.join(cv_sets_directory, feature))\n",
    "        for fold, inchikey_set in folds_mol_set.items():\n",
    "            print(f'\\n{fold}\\n')\n",
    "            for source in source_counts.keys():\n",
    "                source_data = endpoint_df.loc[endpoint_df['ref'].str.contains(source, regex=False)]\n",
    "                source_fold_set = inchikey_set.intersection(set(source_data['inchikey'].tolist()))\n",
    "                source_fold = source_data.loc[source_data['inchikey'].isin(list(source_fold_set))]\n",
    "                prop = (len(source_fold_set) / source_counts[source]) * 100\n",
    "                print(f'{source}: {prop}')\n",
    "\n",
    "                source_fold.to_csv(os.path.join(cv_sets_directory, feature, f'{source}_{fold}.tsv'), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling approach: Split Data (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoints = {'half_life':['logHL', 'Fan'], 'clearance':['logCL', 'Astrazeneca']}\n",
    "features = ['ecfp4', 'rdkit_ecfp4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint in endpoints:\n",
    "    for feature in features:\n",
    "        print(f'\\n\\n ### {endpoint} --> {feature} ### \\n')\n",
    "\n",
    "        # Load data\n",
    "        endpoint_df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', endpoint, f'{endpoints[endpoint][0]}_{feature}_dataset.tsv'), sep='\\t')\n",
    "\n",
    "        # Exclude molecules shared between divergent and homogenous sources\n",
    "        shared_mols = endpoint_df.loc[endpoint_df['ref'].str.contains(endpoints[endpoint][1])]\n",
    "        shared_mols = shared_mols.loc[shared_mols['ref'] != endpoints[endpoint][1]]\n",
    "        endpoint_df_filtered = endpoint_df.loc[~endpoint_df['inchikey'].isin(shared_mols['inchikey'].tolist())]\n",
    "        print(f'Original dataset: {endpoint_df.shape}')\n",
    "        print(f'Filtered dataset: {endpoint_df_filtered.shape}')\n",
    "\n",
    "        # Select divergnet data\n",
    "        divergent_df = endpoint_df_filtered.loc[endpoint_df_filtered['ref'] == endpoints[endpoint][1]]\n",
    "        print(f'Divergent dataset: {divergent_df.shape}')\n",
    "        # Select homogenous data\n",
    "        homogenous_df = endpoint_df_filtered.loc[endpoint_df_filtered['ref'] != endpoints[endpoint][1]]\n",
    "        homogenous_df['ref'] = 'Homogenous'\n",
    "        print(f'Homogenous dataset: {homogenous_df.shape}')\n",
    "\n",
    "        for df in [divergent_df, homogenous_df]:\n",
    "            print(f'\\n{df[\"ref\"].unique()[0]} data')\n",
    "            # Suffle data\n",
    "            df_shuffled = df.sample(frac=1, random_state=seed)\n",
    "            # Split into 5 folds\n",
    "            folds = np.array_split(df_shuffled, 5)\n",
    "            # Save each fold data in a TSV file\n",
    "            cv_sets_scaling_directory = os.path.join(os.getcwd(), '..', 'data', endpoint, 'cv_sets_scaling')\n",
    "            if not os.path.exists(cv_sets_scaling_directory):\n",
    "                os.makedirs(cv_sets_scaling_directory)\n",
    "            if not os.path.exists(os.path.join(cv_sets_scaling_directory, feature)):\n",
    "                os.makedirs(os.path.join(cv_sets_scaling_directory, feature))\n",
    "            for i, fold in enumerate(folds):\n",
    "                print(f'Fold {i+1}: {len(fold)}')\n",
    "                fold.to_csv(os.path.join(cv_sets_scaling_directory, feature, f'{fold[\"ref\"].unique()[0]}_fold{i+1}.tsv'), sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
