{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import GroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "n_splits = 5\n",
    "similarity_threshold= 0.95\n",
    "\n",
    "endpoints = {'half_life':'logHL', 'clearance':'logCL'}\n",
    "divergent_sources =  {'half_life': 'Fan', 'clearance': 'Astrazeneca'}\n",
    "features = ['ecfp4', 'rdkit_ecfp4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precomputing molecule clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_molecule_clusters(similarity_matrix, threshold=0.95):\n",
    "    \"\"\"Given a similarity matrix returns a list with all connected components using the similarity threshold\"\"\"\n",
    "    \n",
    "    # Build graph\n",
    "    G = nx.Graph()\n",
    "    N = len(similarity_matrix)\n",
    "    \n",
    "    # Add edges between molecules with similarity >= threshold\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            if similarity_matrix[i, j] >= threshold:\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    # Find connected components\n",
    "    clusters = list(nx.connected_components(G))\n",
    "\n",
    "    # Sort clusters\n",
    "    clusters = sorted(clusters, key=len, reverse=True)\n",
    "\n",
    "    return clusters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating across endpoints\n",
    "mol2cluster= {}\n",
    "for endpoint in endpoints:\n",
    "    print(f'\\n\\n ### {endpoint} ###\\n')\n",
    "    mol2cluster[endpoint] = {}\n",
    "    \n",
    "    # Read mol descriptors\n",
    "    df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', endpoint, f'{endpoints[endpoint]}_ecfp4_dataset.tsv'), sep='\\t')\n",
    "    ikeys = df['inchikey'].to_numpy()\n",
    "    fps = np.array(df[['ECFP4_%i'%i for i in range(1,1025)]])\n",
    "    \n",
    "    # Calculated similarity matrix\n",
    "    print('Calculating similarity matrix...')\n",
    "    similarity_matrix = 1- squareform(pdist(fps, 'jaccard'))\n",
    "    \n",
    "    # Get clusters\n",
    "    print('Getting molecule clusters...')\n",
    "    clusters = get_molecule_clusters(similarity_matrix, similarity_threshold)\n",
    "    \n",
    "    # Map mols to clusters\n",
    "    for cluster_id, members_ixs in enumerate(clusters):\n",
    "        for member_ix in members_ixs:\n",
    "            ikey = ikeys[member_ix]\n",
    "            mol2cluster[endpoint][ikey] = cluster_id\n",
    "            \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating across endpoints\n",
    "for endpoint in endpoints:\n",
    "    \n",
    "    # Iterate through descriptors\n",
    "    for feature in features:\n",
    "        print(f'\\n\\n ### {endpoint} --> {feature} ### \\n')\n",
    "    \n",
    "        # Read endpoint-feature data\n",
    "        df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', endpoint, f'{endpoints[endpoint]}_{feature}_dataset.tsv'), sep='\\t')\n",
    "      \n",
    "        # Random shuffle data to make folds split randomly\n",
    "        df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        \n",
    "        # Set output file system\n",
    "        cv_sets_directory = os.path.join(os.getcwd(), '..', 'data', endpoint, 'cv_sets', feature)\n",
    "        os.makedirs(cv_sets_directory, exist_ok=True)\n",
    "               \n",
    "        # Count sources\n",
    "        sources_list = [item for item in df['ref'].unique().tolist() if ',' not in item]\n",
    "        source2nmols = dict([(source, len(df.loc[df['ref'].str.contains(source, regex=False)])) for source in sources_list])\n",
    "\n",
    "        # Assigning each mol to a cluster\n",
    "        cluster_list = [str(mol2cluster[endpoint][ikey]) if ikey in mol2cluster[endpoint] else str(ikey).split('-')[0] for ikey in df['inchikey']]\n",
    "        \n",
    "        # Generate splits keeping clusters together\n",
    "        splitter = GroupKFold(n_splits=n_splits)\n",
    "        ixs = np.arange(df.shape[0])\n",
    "        for fold_ix, (train_ixs, test_ixs) in enumerate(splitter.split(ixs,ixs, groups=cluster_list)):\n",
    "            \n",
    "            fold = f'fold{fold_ix+1}'\n",
    "            print(f'\\n{fold}\\n')\n",
    "        \n",
    "            # Getting test-fold dataframe\n",
    "            test_fold_df = df.iloc[test_ixs]\n",
    "          \n",
    "            # Saving test-fold for each source\n",
    "            for source in sources_list:\n",
    "                source_fold = test_fold_df.loc[test_fold_df['ref'].str.contains(source, regex=False)]\n",
    "                source_fold.to_csv(os.path.join(cv_sets_directory, f'{source}_{fold}.tsv'), sep='\\t', index=False)\n",
    "        \n",
    "                # Print proportion of source in fold\n",
    "                n_mols = len(source_fold) \n",
    "                print(source, round(n_mols/source2nmols[source],2))\n",
    "       \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling approach: Split Data (cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate across endpoints\n",
    "for endpoint in endpoints:\n",
    "    divergent_source = divergent_sources[endpoint]\n",
    "    \n",
    "    # Iterate through features\n",
    "    for feature in features:\n",
    "        print(f'\\n\\n### {endpoint} --> {feature} ### \\n')\n",
    "\n",
    "        # Load data\n",
    "        endpoint_df = pd.read_csv(os.path.join(os.getcwd(), '..', 'data', endpoint, f'{endpoints[endpoint]}_{feature}_dataset.tsv'), sep='\\t')\n",
    "\n",
    "        # Get Homogenous and divergent sources\n",
    "        homogenous_df = endpoint_df.loc[endpoint_df['ref'] != divergent_source].copy()\n",
    "        homogenous_df['ref'] = 'Homogenous'\n",
    "        divergent_df = endpoint_df.loc[endpoint_df['ref'] == divergent_source].copy()\n",
    "\n",
    "        # Remove shared molecules\n",
    "        shared_mols = set(endpoint_df.loc[\\\n",
    "                  (endpoint_df['ref'].str.contains(divergent_source))\\\n",
    "                  & (endpoint_df['ref'] != divergent_source)\n",
    "                  ].inchikey)\n",
    "        homogenous_df = homogenous_df.query('inchikey not in @shared_mols').reset_index(drop=True)\n",
    "        divergent_df = divergent_df.query('inchikey not in @shared_mols').reset_index(drop=True)\n",
    "        \n",
    "        for df in [homogenous_df, divergent_df]:\n",
    "            \n",
    "            # Random shuffle data to make folds split randomly\n",
    "            df = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "            \n",
    "            # Set output file system\n",
    "            cv_sets_scaling_directory = os.path.join(os.getcwd(), '..', 'data', endpoint, 'cv_sets_scaling', feature)\n",
    "            os.makedirs(cv_sets_scaling_directory, exist_ok=True)\n",
    "                   \n",
    "            # Count sources\n",
    "            sources_list = [item for item in df['ref'].unique().tolist() if ',' not in item]\n",
    "            source2nmols = dict([(source, len(df.loc[df['ref'].str.contains(source, regex=False)])) for source in sources_list])\n",
    "    \n",
    "            # Assigning each mol to a cluster\n",
    "            cluster_list = [str(mol2cluster[endpoint][ikey]) if ikey in mol2cluster[endpoint] else str(ikey).split('-')[0] for ikey in df['inchikey']]\n",
    "            \n",
    "            # Generate splits keeping clusters together\n",
    "            splitter = GroupKFold(n_splits=n_splits)\n",
    "            ixs = np.arange(df.shape[0])\n",
    "            for fold_ix, (train_ixs, test_ixs) in enumerate(splitter.split(ixs,ixs, groups=cluster_list)):\n",
    "                            \n",
    "                # Getting test-fold dataframe\n",
    "                test_fold_df = df.iloc[test_ixs]\n",
    "             \n",
    "                # Saving test-fold dataframe\n",
    "                df_label = test_fold_df[\"ref\"].iloc[0]\n",
    "                test_fold_df.to_csv(os.path.join(cv_sets_scaling_directory,  f'{df_label}_fold{fold_ix+1}.tsv'), sep='\\t', index=False)\n",
    "              \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AssayInspector",
   "language": "python",
   "name": "assayinspector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
