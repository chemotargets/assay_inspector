{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "from typing import Union, List\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for each ML algorithm\n",
    "\n",
    "CV = 5\n",
    "njobs = -1\n",
    "\n",
    "def objective_xgboost(space, X_train, y_train, seed):\n",
    "    model = XGBRegressor(n_estimators=space['n_estimators'], max_depth=space['max_depth'], \n",
    "                         learning_rate=space['learning_rate'], gamma=space['gamma'],\n",
    "                         min_child_weight=space['min_child_weight'], subsample=space['subsample'],\n",
    "                         colsample_bytree=space['colsample_bytree'], reg_alpha=space['reg_alpha'],\n",
    "                         reg_lambda=space['reg_lambda'], random_state=seed, n_jobs=njobs)\n",
    "    \n",
    "    rmse = -cross_val_score(model, X_train, y_train, cv=CV, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "def objective_svm(space, X_train, y_train):\n",
    "    model = SVR(kernel=space['kernel'], gamma=space['gamma'], C=space['C'], epsilon=space['epsilon'])\n",
    "    \n",
    "    rmse = -cross_val_score(model, X_train, y_train, cv=CV, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "def objective_rf(space, X_train, y_train, seed):\n",
    "    model = RandomForestRegressor(n_estimators=space['n_estimators'], max_depth=space['max_depth'], \n",
    "                                  max_features=space['max_features'], min_samples_split=space['min_samples_split'],\n",
    "                                  random_state=seed, n_jobs=njobs)\n",
    "    \n",
    "    rmse = -cross_val_score(model, X_train, y_train, cv=CV, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "def objective_knn(space, X_train, y_train):\n",
    "    model = KNeighborsRegressor(n_neighbors=space['n_neighbors'], weights=space['weights'], \n",
    "                                metric=space['metric'], n_jobs=njobs)\n",
    "    \n",
    "    rmse = -cross_val_score(model, X_train, y_train, cv=CV, scoring='neg_root_mean_squared_error').mean()\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter search space for each ML algorithm\n",
    "xgboost_space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [50, 100, 200, 500]), \n",
    "    'max_depth': hp.choice('max_depth', [3, 5, 8, 12, 15, 20]), \n",
    "    'learning_rate': hp.choice('learning_rate', [1e-3, 1e-2, 0.05, 0.1, 0.2]),\n",
    "    'gamma': hp.choice ('gamma', [0, 0.1, 0.25, 0.5, 1]),  \n",
    "    'min_child_weight': hp.choice('min_child_weight', [1, 3, 5]), \n",
    "    'subsample': hp.choice('subsample', [0.25, 0.5, 0.7, 1.0]), \n",
    "    'colsample_bytree': hp.choice('colsample_bytree', [0.25, 0.5, 0.7, 1.0]), \n",
    "    'reg_alpha': hp.choice('reg_alpha', [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1]), \n",
    "    'reg_lambda': hp.choice('reg_lambda', [1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1]) \n",
    "}\n",
    "\n",
    "svm_space = {\n",
    "    'kernel': hp.choice('weights', ['linear', 'rbf']),\n",
    "    'gamma': hp.choice('gamma', ['scale', 'auto', 0.001, 0.01, 0.1]),\n",
    "    'C': hp.choice('C', [0.1, 1, 10, 100, 1000]),\n",
    "    'epsilon': hp.choice('epsilon', [0.01, 0.05, 0.1, 0.2])\n",
    "}\n",
    "\n",
    "rf_space = {\n",
    "    'n_estimators': hp.randint('n_estimators', 10, 250),\n",
    "    'max_depth': hp.randint('max_depth', 1, 50),\n",
    "    'max_features': hp.uniform('max_features', 0.1, 0.999),\n",
    "    'min_samples_split': hp.randint('min_samples_split', 2, 25)\n",
    "}\n",
    "\n",
    "knn_space = {\n",
    "    'n_neighbors': hp.randint('n_neighbors', 5, 100),\n",
    "    'weights': hp.choice('weights', ['minkowski', 'euclidean', 'manhattan']),\n",
    "    'metric': hp.choice('metric', ['uniform', 'distance'])\n",
    "}  'metric': hp.choice('metric', metric_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(endpoint: str, feature: str, sources_list: Union[str, List[str]], model_type: str, save_results: bool, experiment_name: str):\n",
    "     \n",
    "    cv_sets_dir = os.path.join(os.getcwd(), '..', 'data', endpoint, 'cv_sets_scaling', feature)\n",
    "    cv_sets_files = os.listdir(cv_sets_dir)\n",
    "\n",
    "    # Initialize metrics variable\n",
    "    metrics = []\n",
    "\n",
    "    # Iterate over seeds:\n",
    "    for seed in range(1,6):\n",
    "        random.seed(seed)\n",
    "        print(f'### Seed {seed} ###')\n",
    "\n",
    "        # Iterate over folds\n",
    "        for i in range(1,6):\n",
    "            print(f'Fold {i}')\n",
    "\n",
    "            # Select test data\n",
    "            test_files = [file for file in cv_sets_files if f'fold{i}' in file]\n",
    "\n",
    "            if len(sources_list) == 1:\n",
    "                # Select and load training data\n",
    "                training_files = [file for file in cv_sets_files if (sources_list[0] in file) and (f'fold{i}' not in file)]\n",
    "                training_data = [pd.read_csv(os.path.join(cv_sets_dir, file), sep=\"\\t\") for file in training_files]\n",
    "                training_df = pd.concat(training_data, ignore_index=True)\n",
    "                # Shuffle data and Set input features and target variable\n",
    "                training_df = training_df.sample(frac=1).reset_index(drop=True)\n",
    "                X_train, y_train = training_df.iloc[:,5:], training_df['value']\n",
    "\n",
    "            else:\n",
    "                X_train_data = []\n",
    "                y_train_data = []\n",
    "                transformers_dict = {}\n",
    "\n",
    "                for source in sources_list:\n",
    "                    # Select and load training data\n",
    "                    source_files = [file for file in cv_sets_files if (source in file) and (f'fold{i}' not in file)]\n",
    "                    source_data = [pd.read_csv(os.path.join(cv_sets_dir, file), sep=\"\\t\") for file in source_files]\n",
    "                    source_df = pd.concat(source_data, ignore_index=True)\n",
    "                    # Set input features and target variable\n",
    "                    X_train, y_train = source_df.iloc[:,5:], source_df['value']\n",
    "                    # Scale target value\n",
    "                    transformer = RobustScaler()\n",
    "                    y_train = pd.Series(transformer.fit_transform(y_train.values.reshape(-1, 1)).flatten(), index=y_train.index)\n",
    "\n",
    "                    X_train_data.append(X_train)\n",
    "                    y_train_data.append(y_train)\n",
    "                    transformers_dict[source] = transformer\n",
    "\n",
    "                # Concatenate features and target variable and Shuffle data\n",
    "                X_train = pd.concat(X_train_data, axis=0)\n",
    "                y_train = pd.concat(y_train_data, axis=0)\n",
    "                X_train, y_train = shuffle(X_train, y_train, random_state=seed)\n",
    "\n",
    "            # Train the ML model\n",
    "            if model_type == 'XGBoost':\n",
    "                # Perform Hyperparameter Tuning\n",
    "                trials=Trials()\n",
    "                best = fmin(fn=lambda space: objective_xgboost(space, X_train, y_train, seed), return_argmin=False, \n",
    "                            space=xgboost_space, algo=tpe.suggest, max_evals=50, timeout=600, trials=trials, rstate=np.random.default_rng(seed))\n",
    "                print(f'Best hyperparameters: {best}')\n",
    "                # Train the final model with the best hyperparameters\n",
    "                model = XGBRegressor(n_estimators=int(best['n_estimators']), max_depth=int(best['max_depth']),\n",
    "                                     learning_rate=float(best['learning_rate']), gamma=float(best['gamma']),\n",
    "                                     min_child_weight=int(best['min_child_weight']), subsample=float(best['subsample']),\n",
    "                                     colsample_bytree=float(best['colsample_bytree']), reg_alpha=float(best['reg_alpha']),\n",
    "                                     reg_lambda=float(best['reg_lambda']), random_state=seed)\n",
    "            \n",
    "            elif model_type == 'SVM':\n",
    "                # Perform Hyperparameter Tuning\n",
    "                trials=Trials()\n",
    "                best = fmin(fn=lambda space: objective_svm(space, X_train[:1000], y_train[:1000]), return_argmin=False, \n",
    "                            space=svm_space, algo=tpe.suggest, max_evals=50, timeout=600, trials=trials, rstate=np.random.default_rng(seed))\n",
    "                print(f'Best hyperparameters: {best}')\n",
    "                # Train the final model with the best hyperparameters\n",
    "                model = SVR(kernel=str(best['kernel']), gamma=float(best['gamma']), \n",
    "                            C=float(best['C']), epsilon=float(best['epsilon']),)\n",
    "                \n",
    "            elif model_type == 'RF':\n",
    "                # Perform Hyperparameter Tuning\n",
    "                trials=Trials()\n",
    "                best = fmin(fn=lambda space: objective_rf(space, X_train, y_train, seed), return_argmin=False, \n",
    "                            space=rf_space, algo=tpe.suggest, max_evals=50, timeout=600, trials=trials, rstate=np.random.default_rng(seed))\n",
    "                print(f'Best hyperparameters: {best}')\n",
    "                # Train the final model with the best hyperparameters\n",
    "                model = RandomForestRegressor(n_estimators=int(best['n_estimators']), max_depth=int(best['max_depth']),\n",
    "                                              max_features=float(best['max_features']), min_samples_split=int(best['min_samples_split']),\n",
    "                                              random_state=seed)\n",
    "\n",
    "            elif model_type == 'KNN':\n",
    "                # Perform Hyperparameter Tuning\n",
    "                trials=Trials()\n",
    "                best = fmin(fn=lambda space: objective_knn(space, X_train, y_train), return_argmin=False, \n",
    "                            space=knn_space, algo=tpe.suggest, max_evals=50, timeout=600, trials=trials, rstate=np.random.default_rng(seed))\n",
    "                print(f'Best hyperparameters: {best}')\n",
    "                # Train the final model with the best hyperparameters\n",
    "                model = KNeighborsRegressor(n_neighbors=int(best['n_neighbors']), weights=str(best['weights']), \n",
    "                                            metric=str(best['metric']))\n",
    "                    \n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Evaluate model performance on each test set\n",
    "            for file in test_files:\n",
    "                # Load test data\n",
    "                test_data = pd.read_csv(os.path.join(cv_sets_dir, file), sep='\\t')\n",
    "                # Set input features and target variable\n",
    "                X_test, y_test = test_data.iloc[:,5:], test_data['value']\n",
    "                # Predict \n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                if len(sources_list) > 1:\n",
    "                    # Perform inverse transformation of the predictions\n",
    "                    transformer = transformers_dict[file[:-10]]\n",
    "                    y_pred = transformer.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "                # Compute metrics\n",
    "                rmse = root_mean_squared_error(y_test, y_pred)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                spearman_corr, spearman_pvalue = spearmanr(y_test, y_pred)  \n",
    "\n",
    "                metrics.append(['_'.join(file.split('.')[0].split('_')[:-1]), seed, i, test_data.shape[0], rmse, mae, r2, spearman_corr, spearman_pvalue])\n",
    "\n",
    "        metrics_df = pd.DataFrame(data=metrics, columns=['ref','seed','fold','n_mols','rmse','mae','r2','spearman_corr','spearman_pvalue'])\n",
    "\n",
    "    # Group by source calculate mean and std for each metric \n",
    "    metrics_summary = metrics_df.groupby('ref')[['rmse','mae','r2','spearman_corr','spearman_pvalue']].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Save results\n",
    "    if save_results:\n",
    "        results_dir = os.path.join(os.getcwd(), '..', 'results_scaling', endpoint, experiment_name)\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "        metrics_df.to_csv(os.path.join(results_dir, f'{model_type}_{feature}_metrics_folds.tsv'), sep='\\t', index=False)\n",
    "        metrics_summary.to_csv(os.path.join(results_dir, f'{model_type}_{feature}_metrics_cv.tsv'), sep='\\t', index=False)\n",
    "    \n",
    "    return metrics_df, metrics_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Half-life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'half_life'\n",
    "features = ['ecfp4', 'rdkit_ecfp4']\n",
    "model_types = ['XGBoost', 'SVM', 'RF', 'KNN']\n",
    "cases_list = [['Homogenous'],\n",
    "              ['Fan'],\n",
    "              ['Homogenous', 'Fan']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    for model in model_types:\n",
    "        for case in cases_list:\n",
    "\n",
    "            print(f'\\n### {\", \".join(case)} --> Features: {feature.upper()}, Model: {model} ###\\n')\n",
    "            \n",
    "            metrics_df, metrics_summary = train_and_evaluate(endpoint, feature=feature, sources_list=case, model_type=model, \n",
    "                                                             save_results=True, experiment_name=f'{\"_\".join(case)}')\n",
    "            print(metrics_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'clearance'\n",
    "features = ['ecfp4', 'rdkit_ecfp4']\n",
    "model_types = ['XGBoost', 'SVM', 'RF', 'KNN']\n",
    "cases_list = [['Homogenous'],\n",
    "              ['Astrazeneca'],\n",
    "              ['Homogenous', 'Astrazeneca']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    for model in model_types:\n",
    "        for case in cases_list:\n",
    "\n",
    "            print(f'\\n### {\", \".join(case)} --> Features: {feature.upper()}, Model: {model} ###\\n')\n",
    "            \n",
    "            metrics_df, metrics_summary = train_and_evaluate(endpoint, feature=feature, sources_list=case, model_type=model, \n",
    "                                                             save_results=True, experiment_name=f'{\"_\".join(case)}')\n",
    "            print(metrics_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
